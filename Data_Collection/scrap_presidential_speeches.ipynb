{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sccraping Presidential Speeches\n",
    "\n",
    "The Miller Center maintains an archive of all presidential speeches. The archive is available at https://millercenter.org/the-presidency/presidential-speeches. The archive contains speeches from all presidents from George Washington to Donald Trump. The archive is organized by president and then by speech. Each speech has a title, date, and transcript. Some speeches also have a video. The archive is a great resource for anyone interested in the history of the United States."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "import re\n",
    "import dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape Links to Speechs from the Miller Center\n",
    "\n",
    "The first thing we need to do is scrape links to all the presidential speeches. The main page uses JavaScript and will only reveal links as you scroll down the page. This, therefore, will require us to use Selenium to scrape this page; as selenium is our only option for scraping JavaScript pages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following works on macos if I have gecko driver in the same folder as the script\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "# load page with Selenium\n",
    "# we need to use selenium because the page loads additional records as you scroll down\n",
    "# if we used requests, we would only get the first page of speeches\n",
    "url = \"https://millercenter.org/the-presidency/presidential-speeches\"\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)\n",
    "\n",
    "#keep scrolling down until page stops loading additional records#\n",
    "pause_scroll = 4\n",
    "last_try = 0\n",
    "initialcoord = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(pause_scroll)\n",
    "    newcoord = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if newcoord == initialcoord:\n",
    "        break\n",
    "    initialcoord = newcoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retrieve urls to all speeches\n",
    "page_source = driver.page_source\n",
    "bsobject_linkpage = bs(page_source, 'html.parser')\n",
    "\n",
    "links = bsobject_linkpage.find_all(\"a\", href= re.compile('presidential-speeches/'))\n",
    "link_list = list()\n",
    "for link in links:\n",
    "    link_specific = link['href']\n",
    "    link_list.append(link_specific)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each URL, scrape the speech\n",
    "\n",
    "For each of the links that we extracted from the main page, we will load the page, and use beutiful soup to extract the speech text, title, date, and president name.\n",
    "\n",
    "NOTE: This will take a long time to run -- possible an hour. The Miller Center has a lot of speeches. Also, you may notice that we pause between pages - this is to make sure we are not hitting the server to hard. Some systems will notice this and reject your requests if you are hitting the server to hard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 50\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     about\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mNo info available\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 50\u001b[0m time\u001b[39m.\u001b[39msleep(page_wait)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# looking at html content...\n",
    "# there is a class called president-name, episode-date, speed-loc, about-sidebar--intro, \n",
    "# presidential-speeches--title, presidential-speeches--title, view-transcript \n",
    "# \n",
    "# view-transcript content may have multiple \"Transcript\" Headers (header 3)\n",
    "# it will also include a title ending in a colon\n",
    "# \n",
    "#scrape the speech#\n",
    "\n",
    "page_wait = 3\n",
    "\n",
    "title, speech, name, date, about = ([] for i in range(5))\n",
    "for index,link in enumerate(link_list):\n",
    "    #access speech page with Selenium and load html source into Beautifulsoup#\n",
    "    driver.get(link_list[index])\n",
    "    driver.find_elements(By.CSS_SELECTOR, 'div[class=\"transcript-inner\"]')\n",
    "    page_source = driver.page_source\n",
    "    bsobject_speechpage = bs(page_source, 'html.parser')\n",
    "\n",
    "    #scrape speech and other properties#\n",
    "    try:\n",
    "        title.append((bsobject_speechpage.find('h2', class_=\"presidential-speeches--title\").text).rstrip())\n",
    "    except AttributeError:\n",
    "        title.append(\"No title available\")\n",
    "        \n",
    "    try:\n",
    "        speech_raw = bsobject_speechpage.find('div', class_=\"transcript-inner\").text\n",
    "    except:\n",
    "        try:\n",
    "            speech_raw = (bsobject_speechpage.find('div', class_=\"view-transcript\").text).rstrip()\n",
    "        except:\n",
    "            speech_raw = \"No speech available\"\n",
    "    speech.append(re.sub(\"Transcript|\\\\n\",\" \",speech_raw))\n",
    "    \n",
    "    try:\n",
    "        name.append((bsobject_speechpage.find('p', class_=\"president-name\").text).rstrip())\n",
    "    except AttributeError:\n",
    "        name.append(\"No name available\")\n",
    "    \n",
    "    try:\n",
    "        date.append((dateparser.parse(bsobject_speechpage.find('p', class_=\"episode-date\").text)))\n",
    "    except AttributeError:\n",
    "        date.append(\"No date available\")\n",
    "        \n",
    "    try:\n",
    "        about.append(bsobject_speechpage.find('div', class_=\"about-sidebar--intro\").text.rstrip())\n",
    "    except AttributeError:\n",
    "        about.append(\"No info available\")\n",
    "    \n",
    "    time.sleep(page_wait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataframe and save to csv\n",
    "\n",
    "Finally, we assemble all out data into a dataframe and save it to a csv file. We will use this csv file in out next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save this to a dataframe and save to a csv file#\n",
    "if len(title) == len(speech) == len(name) == len(date) == len(about):\n",
    "    speeches_presidents = pd.DataFrame({'name':name,'title':title,'date':date,'info':about,'speech':speech}, columns=['name','title','date','info','speech'])\n",
    "    speeches_presidents['speech'] = speeches_presidents['speech'].apply(lambda x: x.replace(\".\",\". \"))\n",
    "    speeches_presidents.to_csv(\"presidential_speeches.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)\n",
    "else:\n",
    "    print(\"Something went wrong with scraping the speeches. Please check the code.\")\n",
    "    \n",
    "    # dump the data to csv files for debugging\n",
    "    df_names = pd.DataFrame({'name':name}) \n",
    "    df_names.to_csv(\"names.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)\n",
    "    \n",
    "    df_titles = pd.DataFrame({'title':title})\n",
    "    df_titles.to_csv(\"titles.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)\n",
    "    \n",
    "    df_dates = pd.DataFrame({'date':date})\n",
    "    df_dates.to_csv(\"dates.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)\n",
    "    \n",
    "    df_infos = pd.DataFrame({'info':about})\n",
    "    df_infos.to_csv(\"infos.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)\n",
    "    \n",
    "    df_speeches = pd.DataFrame({'speech':speech})\n",
    "    df_speeches.to_csv(\"speeches.csv\", encoding=\"utf-8\",quotechar=\"'\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
