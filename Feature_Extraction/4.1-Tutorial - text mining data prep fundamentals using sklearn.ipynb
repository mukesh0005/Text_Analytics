{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining Data Prep\n",
    "\n",
    "### ISM6564\n",
    "\n",
    "**Week04, Part01**\n",
    "\n",
    "&copy; 2023 Dr. Tim Smith\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/prof-tcsmith/ta-f23/blob/main/W04/4.1-Tutorial - text mining data prep fundamentals using sklearn.ipynb#offline=1\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "In this notebook, we will prepare the data for text mining using techniques such as tokenization, stop word removal, and stemming. Also, we will represent our list of documents (in this case, a list of strings) as both a Count Vector and a Term Frequency Inverse Document (TF-ID) matrix. We will use the [scikit-learn](https://scikit-learn.org/stable/) library to perform these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.25.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.11.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\veera\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# we will use spacy for lemmatization (it's much better than nltk)\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# we will use sklearn for feature extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# we will use sklearn for dimensionality reduction\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start with a corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the corpus of documents\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"If he cares about caring, then he should care about caring about caring.\",\n",
    "    \"If he began to care, then he should begin to care about caring about caring.\",\n",
    "    \"123 the world is large 32.34\",\n",
    "    'He stripped the striped paint by stripping the first coat of paint.'\n",
    "]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a term by document matrix\n",
    "\n",
    "TfidfVectorizer and CountVectorizer both are methods for converting text data into vectors as model can process only numerical data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using CountVectorizer\n",
    "\n",
    "In CountVectorizer we only count the number of times a word appears in the document which results in biasing in favour of most frequent words. this ends up in ignoring rare words which could have helped is in processing our data more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation and numbers\n",
    "corpus = [re.sub(r'[^a-zA-Z ]+', '', doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>began</th>\n",
       "      <th>begin</th>\n",
       "      <th>care</th>\n",
       "      <th>cares</th>\n",
       "      <th>caring</th>\n",
       "      <th>coat</th>\n",
       "      <th>document</th>\n",
       "      <th>large</th>\n",
       "      <th>paint</th>\n",
       "      <th>second</th>\n",
       "      <th>striped</th>\n",
       "      <th>stripped</th>\n",
       "      <th>stripping</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   began  begin  care  cares  caring  coat  document  large  paint  second  \\\n",
       "0      0      0     0      0       0     0         1      0      0       0   \n",
       "1      0      0     0      0       0     0         2      0      0       1   \n",
       "2      0      0     1      1       3     0         0      0      0       0   \n",
       "3      1      1     2      0       2     0         0      0      0       0   \n",
       "4      0      0     0      0       0     0         0      1      0       0   \n",
       "5      0      0     0      0       0     1         0      0      2       0   \n",
       "\n",
       "   striped  stripped  stripping  world  \n",
       "0        0         0          0      0  \n",
       "1        0         0          0      0  \n",
       "2        0         0          0      0  \n",
       "3        0         0          0      0  \n",
       "4        0         0          0      1  \n",
       "5        1         1          1      0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CountVectorizer will covert to lowercase, remove punctuation, and remove stop words - to \n",
    "# remove other things, such as numbers, use the token_pattern parameter\n",
    "vectorizer = CountVectorizer(stop_words='english', lowercase=True) # this will handle all text cleaning, except removing numbers\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TficVectorizer\n",
    "\n",
    "To overcome this problem (over emphasis on high frequency), we use TfidfVectorizer .\n",
    "\n",
    "In TfidfVectorizer we consider overall document weightage of a word. It helps us in dealing with most frequent words. Using it we can penalize them. TfidfVectorizer weights the word counts by a measure of how often they appear in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>began</th>\n",
       "      <th>begin</th>\n",
       "      <th>care</th>\n",
       "      <th>cares</th>\n",
       "      <th>caring</th>\n",
       "      <th>coat</th>\n",
       "      <th>document</th>\n",
       "      <th>large</th>\n",
       "      <th>paint</th>\n",
       "      <th>second</th>\n",
       "      <th>striped</th>\n",
       "      <th>stripped</th>\n",
       "      <th>stripping</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.295049</td>\n",
       "      <td>0.359809</td>\n",
       "      <td>0.885146</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.36812</td>\n",
       "      <td>0.36812</td>\n",
       "      <td>0.603728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.603728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.353553</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     began    begin      care     cares    caring      coat  document  \\\n",
       "0  0.00000  0.00000  0.000000  0.000000  0.000000  0.000000    1.0000   \n",
       "1  0.00000  0.00000  0.000000  0.000000  0.000000  0.000000    0.8538   \n",
       "2  0.00000  0.00000  0.295049  0.359809  0.885146  0.000000    0.0000   \n",
       "3  0.36812  0.36812  0.603728  0.000000  0.603728  0.000000    0.0000   \n",
       "4  0.00000  0.00000  0.000000  0.000000  0.000000  0.000000    0.0000   \n",
       "5  0.00000  0.00000  0.000000  0.000000  0.000000  0.353553    0.0000   \n",
       "\n",
       "      large     paint    second   striped  stripped  stripping     world  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.520601  0.000000  0.000000   0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
       "3  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  \n",
       "4  0.707107  0.000000  0.000000  0.000000  0.000000   0.000000  0.707107  \n",
       "5  0.000000  0.707107  0.000000  0.353553  0.353553   0.353553  0.000000  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like CountVectorizer, TfidfVectorizer will covert to lowercase, remove punctuation, and remove \n",
    "# stop words - to remove other things, such as numbers, use the token_pattern parameter\n",
    "vectorizer = TfidfVectorizer(stop_words='english', lowercase=True)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Word Lemmatization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we might benefit from finding the lemma of a word. For example, the words \"beginning\", \"begun\", and \"begins\" are all related to the same concept or begin. We can use the NLTK's WordNetLemmatizer to reduce words to their lemmas.\n",
    "\n",
    "> Stemming is a process that stems or removes last few characters from a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. For instance, stemming the word 'Caring' would return 'Car' whereas lemmatization would return 'Care'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.1\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "# Replace 'path/to/es_core_news_lg-3.1.0.tar.gz' with the actual path to your model file\n",
    "model_file = './models/es_core_news_lg-3.1.0.tar.gz'\n",
    "\n",
    "# Extract the model\n",
    "with tarfile.open(model_file, 'r:gz') as tar:\n",
    "    tar.extractall(path='./models/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\veera\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'es_core_news_lg' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.6.1). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "load_model = spacy.load(\"./models/es_core_news_lg-3.1.0/es_core_news_lg\\es_core_news_lg-3.1.0/\") \n",
    "# python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is the first document',\n",
       " 'This document is the second document',\n",
       " 'if haber car about caring then haber should care about caring about caring',\n",
       " 'if haber begar to care then haber should begin to care about caring about caring',\n",
       " '  the world is large',\n",
       " 'haber stripped the striped paint by stripping the first coat of paint']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "    doc = re.sub(r'[^a-zA-Z ]+', '', doc) # remove punctuation and numbers\n",
    "    cleaned_corpus.append(\" \".join([token.lemma_ for token in load_model(doc)]))\n",
    "    \n",
    "cleaned_corpus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the TfidfVectorizer to convert our new lematized corpus into a matrix of TF-IDF features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>begar</th>\n",
       "      <th>begin</th>\n",
       "      <th>car</th>\n",
       "      <th>care</th>\n",
       "      <th>caring</th>\n",
       "      <th>coat</th>\n",
       "      <th>document</th>\n",
       "      <th>haber</th>\n",
       "      <th>large</th>\n",
       "      <th>paint</th>\n",
       "      <th>second</th>\n",
       "      <th>striped</th>\n",
       "      <th>stripped</th>\n",
       "      <th>stripping</th>\n",
       "      <th>world</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.8538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520601</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.322055</td>\n",
       "      <td>0.264089</td>\n",
       "      <td>0.792268</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.445925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.327973</td>\n",
       "      <td>0.327973</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.537886</td>\n",
       "      <td>0.537886</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.454120</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343416</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.237751</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.686831</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.343416</td>\n",
       "      <td>0.343416</td>\n",
       "      <td>0.343416</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      begar     begin       car      care    caring      coat  document  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    1.0000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.8538   \n",
       "2  0.000000  0.000000  0.322055  0.264089  0.792268  0.000000    0.0000   \n",
       "3  0.327973  0.327973  0.000000  0.537886  0.537886  0.000000    0.0000   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000    0.0000   \n",
       "5  0.000000  0.000000  0.000000  0.000000  0.000000  0.343416    0.0000   \n",
       "\n",
       "      haber     large     paint    second   striped  stripped  stripping  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.520601  0.000000  0.000000   0.000000   \n",
       "2  0.445925  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "3  0.454120  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "4  0.000000  0.707107  0.000000  0.000000  0.000000  0.000000   0.000000   \n",
       "5  0.237751  0.000000  0.686831  0.000000  0.343416  0.343416   0.343416   \n",
       "\n",
       "      world  \n",
       "0  0.000000  \n",
       "1  0.000000  \n",
       "2  0.000000  \n",
       "3  0.000000  \n",
       "4  0.707107  \n",
       "5  0.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Like CountVectorizer, TfidfVectorizer will covert to lowercase, remove punctuation, and remove \n",
    "# stop words - to remove other things, such as numbers, use the token_pattern parameter\n",
    "vectorizer = TfidfVectorizer(token_pattern=r'[a-zA-Z]+', stop_words='english', lowercase=True)\n",
    "\n",
    "X = vectorizer.fit_transform(cleaned_corpus)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SVD for dimension reduction\n",
    "\n",
    "Let's apply SVD to reduce the dimensionality of our data. \n",
    "\n",
    "> NOTE: Recall that the input dimensions will be the number of unique words in the corpus. With large corpa, the number of unique words can be very large, and thus the dimensionality of the data can be very large. Four our small corpus, the problem of high dimensionality not really a concern. With large corpo, you may need to reduce the dimensionality of the data to make it more manageable for the machine learning algorithms (especially clustering and neural networks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you are performing Latent Semantic Analysis, recommended number of components is 100\n",
    "\n",
    "svd = TruncatedSVD(n_components=5, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 9.62756515e-01,  5.42412335e-17,  8.27567069e-20,\n",
       "         1.24321210e-20,  1.70663257e-19],\n",
       "       [ 9.62756515e-01,  6.14863764e-16, -2.44398773e-18,\n",
       "        -4.32811627e-18, -1.02360888e-18],\n",
       "       [-5.11464708e-16,  9.31805718e-01,  2.64093899e-14,\n",
       "        -1.30890120e-01, -3.38534902e-01],\n",
       "       [-5.59589666e-16,  9.32115333e-01,  2.57798355e-14,\n",
       "        -1.28347458e-01,  3.38656073e-01],\n",
       "       [ 2.92142898e-18, -3.86857021e-17,  1.00000000e+00,\n",
       "         2.01343516e-13, -8.96435710e-17],\n",
       "       [-1.27286656e-16,  2.49488256e-01, -1.94571024e-13,\n",
       "         9.68377431e-01, -8.72828865e-04]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_svd = svd.fit_transform(X)\n",
    "X_svd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_svd.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>svd0000</th>\n",
       "      <th>svd0001</th>\n",
       "      <th>svd0002</th>\n",
       "      <th>svd0003</th>\n",
       "      <th>svd0004</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.627565e-01</td>\n",
       "      <td>5.424123e-17</td>\n",
       "      <td>8.275671e-20</td>\n",
       "      <td>1.243212e-20</td>\n",
       "      <td>1.706633e-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.627565e-01</td>\n",
       "      <td>6.148638e-16</td>\n",
       "      <td>-2.443988e-18</td>\n",
       "      <td>-4.328116e-18</td>\n",
       "      <td>-1.023609e-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.114647e-16</td>\n",
       "      <td>9.318057e-01</td>\n",
       "      <td>2.640939e-14</td>\n",
       "      <td>-1.308901e-01</td>\n",
       "      <td>-3.385349e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.595897e-16</td>\n",
       "      <td>9.321153e-01</td>\n",
       "      <td>2.577984e-14</td>\n",
       "      <td>-1.283475e-01</td>\n",
       "      <td>3.386561e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.921429e-18</td>\n",
       "      <td>-3.868570e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.013435e-13</td>\n",
       "      <td>-8.964357e-17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-1.272867e-16</td>\n",
       "      <td>2.494883e-01</td>\n",
       "      <td>-1.945710e-13</td>\n",
       "      <td>9.683774e-01</td>\n",
       "      <td>-8.728289e-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        svd0000       svd0001       svd0002       svd0003       svd0004\n",
       "0  9.627565e-01  5.424123e-17  8.275671e-20  1.243212e-20  1.706633e-19\n",
       "1  9.627565e-01  6.148638e-16 -2.443988e-18 -4.328116e-18 -1.023609e-18\n",
       "2 -5.114647e-16  9.318057e-01  2.640939e-14 -1.308901e-01 -3.385349e-01\n",
       "3 -5.595897e-16  9.321153e-01  2.577984e-14 -1.283475e-01  3.386561e-01\n",
       "4  2.921429e-18 -3.868570e-17  1.000000e+00  2.013435e-13 -8.964357e-17\n",
       "5 -1.272867e-16  2.494883e-01 -1.945710e-13  9.683774e-01 -8.728289e-04"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_svd, columns=[f\"svd{num:04}\" for num in range(0,X_svd.shape[1])])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have taken the 14 dimensions of input and reduced these down to 5 dimensions. This is a 64% reduction in the number of dimensions. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are ready to use this data as input \n",
    "\n",
    "Our text data is now ready to be used in a model. If we have other classification meta data (for instance, news category, or customer or not, etc.), we can create predictive models using machine learning techniques. If we don't have any tags/etadata, we can use this data to cluster the documents - or, go through the documents manually and tag them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
